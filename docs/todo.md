## Notes

### 21 Nov
- Search for text book assignments on regular expressions
- Draft a set of user test scenarios
- Improve the feedback messages from the cli
- Search for paper or articles for cli/llm/user interface

### 17 Dec

- Search for text book assignments on regular expressions
  - https://regex.sketchengine.eu/basic-exercises.html These exercises are providing a problem in which you have to write a regular expression to solve it. There is no error in the command
- Draft a set of user test scenarios
  - Check user-tests.md for the user test scenarios
- Improve the feedback messages from the cli
  - TODO: Think about how the feedback could be even more concise
- Search for paper or articles for cli/llm/user interface
  - Check research.md for the papers and articles
- IDEA: Consider calling function to discover the working directory like ls or pwd

### 19 Dec
- Check magick lib
- Maybe add input also
- Draft metrics for user tests
  - Try to find what is the best metric upon use case
  - Frustration?
  - Time to correct?
  - Number of errors/tries?
  - Time?

### 9 Jan
- Test with 2 users for feedback and collect
- Ask DI for the thesis timeline
- Battle test analytics to make sure it is working for the final experiment

### 30 Jan
1. Maybe we can measure the time manually and just make going to the next step more prominent, since this is a small scale experiment
2. Decide upon 2 or 3 cases for comparative testing
3. How much info would be ideal? Maybe add it to questioner
4. Add prompts with #
5. Add more cases to make it bigger even similar example to get more info
6. Start categorizing the test cases
7. Adding more contextual problems
8. Decide "between or within" groups - Evaluate learning effect, eliminate personal diffs
9. Sample is based on users or on interactions?

### 20 Feb
1. Add a table with more parameters in the thesis doc (price, os to be installed, readline)
2. UIST -> to prevent the experiment.

### 3 May - new TODOs:
1. Implement a timeout
2. ✅ Get from research the commands we gather and update user-test
3. Deploy the experiment to digital ocean
4. ✅ Create a list of who is going to run this
  ----------
  1. Panos - Pilot
  2. Oikonomou - Pilot
  ----------
  1. Anastasis - Actual Test
  2. Zach - Actual Test
  3. Koutsafikis - Actual Test
  4. Maria - Actual Test
  5. Dennis - Actual Test
  ----------
5. Run the experiment
6. Analyze results
7. Write the final paragraphs about the results we found
8. ✅ Export questionaires to be more customizable - Extract to json?
9. ✅ Remove indication of LLM enabled vs Traditional
10. ✅ Remove effectivness from questionaire

11. Shoud we move this to satisfaction? ✔ Rate the effectiveness in handling errors
  Question: Is it right that while we are within group we ask people about the satisfaction?

12. End up with 5 questions for personal satisfaction
13. Increase difficulty in all commands since we will be testing in more experienced users