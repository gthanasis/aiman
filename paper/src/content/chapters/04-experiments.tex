\section{Test Environment \& Setup}

We will use a within-subjects experimental design where each participant performs tasks under both conditions: (A) using the traditional CLI and (B) using the AI-assisted CLI. In other words, every participant experiences both interfaces, allowing direct comparison of their performance with and without AI support. This design controls for individual differences in skill, since each person serves as their own control.

To avoid order effects (learning or fatigue influencing results), the sequence of conditions will be counterbalanced:
\begin{itemize}
	\item Half of the participants will use the Traditional CLI first, then the AI-assisted CLI.
	\item The other half will use the AI-assisted CLI first, then the Traditional CLI.
\end{itemize}

This counterbalancing ensures that any overall improvement due to practice is distributed across both conditions, and we can more confidently attribute differences in performance to the interface rather than task familiarity.

\subsection{Participants}

\textbf{Target Users:} The study will involve experienced software engineers (5–10 years of professional experience) as the primary participant group. These users are proficient with computers and likely familiar with CLI basics, which helps focus the study on efficiency gains and subtle improvements.

\textbf{Sample Size:} We aim for a sufficient number of participants to yield meaningful comparisons. Within-subjects designs typically require fewer participants than between-subject because each person serves as their own control. We plan to recruit N participants (e.g. 5-10), balancing practical feasibility with statistical power.

All participants should have basic familiarity with using a terminal (to ensure they can attempt the tasks), but not necessarily expert-level CLI mastery. Prior to the experiment, each participant will fill out a pre-survey detailing their background (years of experience, self-rated CLI proficiency, etc.), which will help contextualize the results.

\subsection{Apparatus and Environment}

\textbf{Operating System:} The experiment will be conducted on a Unix-like OS (Linux or macOS) as these provide a standard CLI environment. All participants will use the same type of system to ensure consistency (e.g. a provided MacBook or Linux VM configured for the study).

\textbf{Traditional CLI Tool:} The baseline interface is the native terminal (e.g. Bash or Zsh shell on a typical Terminal app). This provides no special assistance beyond standard shell features.

\textbf{AI-Assisted CLI Tool:} This tool integrates a generative AI assistant into the terminal, which can detect command errors or incomplete commands and suggest corrections or completions in real-time. Key features of the AI CLI include:
\begin{itemize}
	\item Syntax correction and suggestions when a command fails or is likely incorrect
	\item Enhanced help or examples when the user is unsure of a command
\end{itemize}

\textbf{Interactive Task Harness:} An interactive testing harness (included in the AI CLI codebase) will be used to present tasks and record data. This harness can present a task description to the user in the terminal, accept their command input, and automatically log outcomes:
\begin{itemize}
	\item In traditional mode, it simply records the commands entered and whether the task was completed successfully
	\item In AI-assisted mode, it also logs AI suggestions made and whether the user accepted or ignored them
\end{itemize}

\textbf{Hardware and Setup:} All sessions will be conducted on similar hardware to avoid performance differences. Internet access will be available if required for certain tasks, and the network conditions will be consistent. Screen and command logging tools will be active.

Before starting, each system will be reset to a clean state (e.g. clearing any command history that could give hints) and loaded with any files or directories needed for the tasks.

\subsection{Data Collection \& Evaluation Metrics}

Multiple quantitative metrics will be collected to evaluate performance on each task and across conditions. Table~\ref{tab:metrics} summarizes the key metrics and their definitions:

\begin{table}[h]
	\centering
	\caption{Key Metrics for Evaluation}
	\label{tab:metrics}
	\begin{tabular}{|l|p{6cm}|p{4cm}|}
		\hline
		\textbf{Metric}              & \textbf{Description and Definition}                                                                                                                                         & \textbf{Collection Method}                                                                                                                  \\
		\hline
		Task Completion Time         & The time (in seconds) from when a task is presented to the participant until it is successfully completed. Lower times indicate higher efficiency.                          & Automatically tracked by the testing harness (timestamps when task is given and when correct output is achieved)                            \\
		\hline
		Success Rate                 & Whether the participant successfully completes the task without irrecoverable error. This can be binary (success/failure) or a percentage if partial credit is considered.  & Derived from logs (whether the correct command was eventually executed)                                                                     \\
		\hline
		Number of Errors/Corrections & How many incorrect attempts or corrections were made before completing the task. This includes syntax errors, mis-typed commands, and uses of the AI's correction features. & From the command log: count the number of times a command had to be re-entered or corrected                                                 \\
		\hline
		Learning Retention           & An indicator of how well users retain command knowledge or improved skill over time. This can be measured by comparing performance on repeated tasks.                       & Computed from repeated task data. For example, if a user initially needed help but later completed it correctly, we measure the improvement \\
		\hline
		User Satisfaction            & The subjective satisfaction of the user with the interface and their performance. This encompasses ease of use, confidence, and frustration level.                          & Measured via post-task Likert scale questionnaire (e.g., rating statements like "Using this CLI was easy")                                  \\
		\hline
	\end{tabular}
\end{table}

All these metrics align with standard usability measures: efficiency (time), effectiveness (success and errors), and satisfaction (subjective feedback). Data will be collected through a combination of automated logging and manual observation.

\section{User Testing Scenarios}

This section presents the task-based experimental framework developed to evaluate the impact of AI-assisted CLI interactions on user performance during command repair. The task set was constructed to reflect common command-line operations—such as file navigation, text processing, and system inspection—based on empirical usage patterns identified in large-scale studies of Unix shell behavior and established pedagogical frameworks.

Each task includes a broken command intentionally designed to elicit a syntax or semantic error. These errors were informed by prior human-computer interaction research that identified common CLI pitfalls for novice users, including syntactic memorization burdens, abstract command mappings, and high failure rates in manual correction.

Each scenario includes:
\begin{itemize}
	\item A brief description of the user's intent
	\item The incorrect (broken) command
	\item One or more valid corrected commands
	\item The skill or concept being tested
\end{itemize}

\subsection{File Navigation Scenarios}

\textbf{Scenario 1: Change to a directory}
\begin{itemize}
	\item \textit{Intent:} Navigate to the Documents directory
	\item \textit{Broken Command:} \texttt{cd documents}
	\item \textit{Correct Command:} \texttt{cd Documents}
	\item \textit{Tests:} Case sensitivity in path names
\end{itemize}

\textbf{Scenario 2: List files with details}
\begin{itemize}
	\item \textit{Intent:} List all files with detailed information
	\item \textit{Broken Command:} \texttt{ls detail}
	\item \textit{Correct Commands:} \texttt{ls -l}, \texttt{ls -la}
	\item \textit{Tests:} Use of flags for detailed listing
\end{itemize}

\textbf{Scenario 3: Show current path}
\begin{itemize}
	\item \textit{Intent:} Display the current working directory
	\item \textit{Broken Command:} \texttt{print working directory}
	\item \textit{Correct Command:} \texttt{pwd}
	\item \textit{Tests:} Mapping natural language to standard command
\end{itemize}

\subsection{File Management Scenarios}

\textbf{Scenario 4: Create directory}
\begin{itemize}
	\item \textit{Intent:} Make a directory called "projects"
	\item \textit{Broken Command:} \texttt{mkdir -create projects}
	\item \textit{Correct Command:} \texttt{mkdir projects}
	\item \textit{Tests:} Misused flag for directory creation
\end{itemize}

\textbf{Scenario 5: Create a file}
\begin{itemize}
	\item \textit{Intent:} Create an empty file named notes.txt
	\item \textit{Broken Command:} \texttt{touch new notes.txt}
	\item \textit{Correct Command:} \texttt{touch notes.txt}
	\item \textit{Tests:} Extra keyword removal
\end{itemize}

\textbf{Scenario 6: Copy file with versioning}
\begin{itemize}
	\item \textit{Intent:} Copy file.txt to a backup
	\item \textit{Broken Command:} \texttt{cp file.txt --backup}
	\item \textit{Correct Commands:} \texttt{cp file.txt backup.txt}, \texttt{cp file.txt file.txt.backup}
	\item \textit{Tests:} Misunderstanding of non-existent flags
\end{itemize}

\subsection{File Search and Text Search Scenarios}

\textbf{Scenario 7: Find .log files}
\begin{itemize}
	\item \textit{Intent:} Search for .log files
	\item \textit{Broken Command:} \texttt{find . --name*.log}
	\item \textit{Correct Command:} \texttt{find . -name "*.log"}
	\item \textit{Tests:} Flag format, quoting, wildcards
\end{itemize}

\textbf{Scenario 8: Search text in logs}
\begin{itemize}
	\item \textit{Intent:} Search for "ERROR" in logs
	\item \textit{Broken Command:} \texttt{grep ERROR}
	\item \textit{Correct Commands:} \texttt{grep "ERROR" logfile}, \texttt{grep "ERROR" *.log}
	\item \textit{Tests:} Required arguments and search context
\end{itemize}

\subsection{File Viewing Scenarios}

\textbf{Scenario 9: Display file contents}
\begin{itemize}
	\item \textit{Intent:} Output contents of file.txt
	\item \textit{Broken Command:} \texttt{cat --show file.txt}
	\item \textit{Correct Command:} \texttt{cat file.txt}
	\item \textit{Tests:} Misunderstood flag
\end{itemize}

\textbf{Scenario 10: Page through a large file}
\begin{itemize}
	\item \textit{Intent:} View a large file one screen at a time
	\item \textit{Broken Command:} \texttt{scroll file.txt}
	\item \textit{Correct Commands:} \texttt{less file.txt}, \texttt{more file.txt}
	\item \textit{Tests:} Recognizing appropriate paging tools
\end{itemize}

\textbf{Scenario 11: View first N lines}
\begin{itemize}
	\item \textit{Intent:} Show the first 10 lines of a file
	\item \textit{Broken Command:} \texttt{head --lines=10 file.txt}
	\item \textit{Correct Commands:} \texttt{head -n 10 file.txt}, \texttt{head -10 file.txt}
	\item \textit{Tests:} Flag syntax and numeric input
\end{itemize}

\subsection{Text Processing Scenarios}

\textbf{Scenario 12: Line count in a file}
\begin{itemize}
	\item \textit{Intent:} Count lines in file.txt
	\item \textit{Broken Command:} \texttt{wc -k file.txt}
	\item \textit{Correct Commands:} \texttt{wc -l file.txt}, \texttt{cat file.txt | wc -l}
	\item \textit{Tests:} Incorrect flag, valid alternatives
\end{itemize}

\textbf{Scenario 13: Sort lines in a file}
\begin{itemize}
	\item \textit{Intent:} Alphabetically sort a text file
	\item \textit{Broken Command:} \texttt{sort --alpha names.txt}
	\item \textit{Correct Command:} \texttt{sort names.txt}
	\item \textit{Tests:} Use of invented flags
\end{itemize}

\textbf{Scenario 14: Extract CSV column}
\begin{itemize}
	\item \textit{Intent:} Get the second column from data.csv
	\item \textit{Broken Command:} \texttt{cut column 2 data.csv}
	\item \textit{Correct Commands:} Variants of \texttt{cut -d "," -f2 data.csv}
	\item \textit{Tests:} Quoting, delimiters, column syntax
\end{itemize}

\subsection{System Information Scenarios}

\textbf{Scenario 15: Directory disk usage}
\begin{itemize}
	\item \textit{Intent:} Show size of current directory
	\item \textit{Broken Command:} \texttt{du h .}
	\item \textit{Correct Commands:} \texttt{du -h .}, \texttt{du -sh .}
	\item \textit{Tests:} Flag formatting
\end{itemize}

\textbf{Scenario 16: Disk space usage}
\begin{itemize}
	\item \textit{Intent:} View human-readable disk space
	\item \textit{Broken Command:} \texttt{df size -h}
	\item \textit{Correct Commands:} \texttt{df -h}, \texttt{df -Th}
	\item \textit{Tests:} Flag usage and order
\end{itemize}

\textbf{Scenario 17: Display current user}
\begin{itemize}
	\item \textit{Intent:} Check which user is logged in
	\item \textit{Broken Command:} \texttt{user}
	\item \textit{Correct Command:} \texttt{whoami}
	\item \textit{Tests:} Mapping to correct utility
\end{itemize}

\subsection{Process Management Scenarios}

\textbf{Scenario 18: List running processes}
\begin{itemize}
	\item \textit{Intent:} Show all processes
	\item \textit{Broken Command:} \texttt{ps --all}
	\item \textit{Correct Commands:} \texttt{ps aux}, \texttt{ps -ef}
	\item \textit{Tests:} Platform differences, common alternatives
\end{itemize}

\textbf{Scenario 19: Kill a process}
\begin{itemize}
	\item \textit{Intent:} Terminate process ID [1234]
	\item \textit{Broken Command:} \texttt{kill --terminate [1234]}
	\item \textit{Correct Commands:} \texttt{kill 1234}, \texttt{kill -9 1234}, \texttt{kill -TERM [1234]}
	\item \textit{Tests:} Flag syntax and force variants
\end{itemize}

\subsection{Networking and Archiving Scenarios}

\textbf{Scenario 20: Download from URL}
\begin{itemize}
	\item \textit{Intent:} Download file from web
	\item \textit{Broken Command:} \texttt{curl get http://example.com/file}
	\item \textit{Correct Commands:} \texttt{curl -O http://example.com/file}, \texttt{wget http://example.com/file}
	\item \textit{Tests:} Tool equivalence, parameter understanding
\end{itemize}

\textbf{Scenario 21: Create archive}
\begin{itemize}
	\item \textit{Intent:} Compress a folder
	\item \textit{Broken Command:} \texttt{tar zip folder/}
	\item \textit{Correct Commands:} \texttt{tar -cvzf archive.tar.gz folder/}, \texttt{tar -czvf archive.tar.gz folder/}
	\item \textit{Tests:} Correct archiving flags
\end{itemize}

\textbf{Scenario 22: Duplicate output to file and screen}
\begin{itemize}
	\item \textit{Intent:} Write output to file and screen
	\item \textit{Broken Command:} \texttt{ls > output.txt --show}
	\item \textit{Correct Command:} \texttt{ls | tee output.txt}
	\item \textit{Tests:} Redirection versus piping
\end{itemize}

Each scenario is phrased as a goal for the user (e.g., "Download this webpage to a file", "Find all text files in this directory"). Participants will not be given the exact command, but they will have enough context to know what they need to accomplish. The tasks are designed to be achievable for someone with moderate CLI knowledge, but each has pitfalls where mistakes are common.

\section{Procedure Flow}

Each participant will go through the following phases:

\textbf{Introduction \& Consent:} The researcher welcomes the participant, explains the study procedure, and obtains informed consent. Basic demographic and experience information is collected (if not already via a pre-survey).

\textbf{Training/Tutorial:} Before each interface, participants get a brief tutorial:
\begin{itemize}
	\item For the AI-assisted CLI, they will be shown how the tool works: for example, how it provides suggestions after an error, how to accept a suggestion, and any special commands to invoke help. They may practice with one simple example task to familiarize themselves with the AI features.
	\item The traditional CLI likely needs less introduction, but to ensure fairness, participants can be reminded of available manual help resources (e.g. man pages or --help flags) if they get stuck.
\end{itemize}

\textbf{Baseline Task Set (Traditional CLI):} In one of the two sessions (depending on counterbalancing), the participant uses the normal terminal to attempt a set of tasks (e.g. Tasks 1–11). They will:
\begin{itemize}
	\item Read the task description provided by the experimenter or shown on-screen
	\item Attempt to execute the task in the terminal. They may use any knowledge they have, including --help or manual pages, but no AI assistance is available
	\item The system will log the time taken and whether the outcome is correct
	\item If they are truly stuck, they are allowed to ask for a hint or give up, but this will be recorded (with a reasonable maximum time per task, such as 5 minutes, to avoid endless frustration)
\end{itemize}

\textbf{Experimental Task Set (AI-Assisted CLI):} In the other session, the participant uses the AI-enabled terminal to perform another set of tasks (e.g. Tasks 12–22, or a differently ordered set to cover similar scenarios). They follow the same process, except now the AI assistant is active. If they make a mistake or pause, the AI might provide a suggestion or auto-correct common errors. The participant can choose to accept the suggestion, modify it, or ignore it. The system logs all AI interactions (e.g., number of suggestions shown, which suggestions were accepted).

\textbf{Repeated Tasks for Retention (Learning Evaluation):} To evaluate learning retention, some tasks may be repeated. For example, after using the AI-assisted CLI, we might ask the participant to redo one or two key tasks in the traditional CLI without assistance (possibly tasks they struggled with initially or tasks that the AI helped correct). The idea is to see if the correction or knowledge provided by the AI was retained. Improved performance on the second attempt (faster or fewer errors) would indicate learning. This can be done either at the end of the session or in a follow-up session (even a few days later to test longer-term retention).

\textbf{Post-Task Survey \& Interview:} After completing all tasks in both conditions, participants will fill out a post-experiment questionnaire. This survey will capture subjective feedback:
\begin{itemize}
	\item \textit{Satisfaction and Preference:} Which interface did they prefer? How satisfied are they with their performance in each? (Likert scale ratings for statements like "I felt confident using the CLI" in each condition.)
	\item \textit{Perceived Efficiency:} Did they feel faster or more efficient with AI assistance?
	\item \textit{Learning and Confidence:} Do they feel the AI tool helped them learn CLI commands, or did it make them reliant on suggestions?
	\item \textit{Comments:} Open-ended feedback on what they liked or disliked, any suggestions for improvement.
\end{itemize}

A short structured interview may follow to discuss their experience, gather qualitative observations (e.g., "Did you trust the AI suggestions?", "How was the experience of fixing errors with vs. without AI?", etc.).

Throughout the sessions, the experimenter will observe and take notes (without interfering). Notable observations, such as signs of frustration (sighs, repeated errors) or ease (quick task completion), will be recorded as qualitative data.

\section{Analysis of Results}

Once the data is collected, we will perform both quantitative and qualitative analysis to address the research goals.

\subsection{Efficiency Analysis}

For each task, we'll compare how long it took to complete using the traditional CLI versus the AI-assisted CLI. Since the same people use both, we'll do paired comparisons—like a paired t-test (if the data looks normal) or the Wilcoxon signed-rank test (if not)—to see if the difference in times is significant.

We expect the AI CLI to help more with harder tasks, especially those with tricky syntax or ones where people usually pause to think. For simple tasks (like listing files), the difference might be small.

\subsection{Effectiveness and Error Analysis}

We will examine success rates and error counts:
\begin{itemize}
	\item Success rate likely will be high for both (since users can eventually complete tasks), but the number of initial errors and retries will differ.
	\item We'll calculate the average number of errors per task in each condition.
	\item Using the logs, we might also classify error types (e.g., syntax errors, wrong command approach, etc.) to see what kinds of mistakes the AI helps with most.
\end{itemize}

\subsection{Learning and Adaptation Analysis}

To address the learning objective, we will analyze the learning curve in several ways:
\begin{itemize}
	\item \textbf{Within-Session Adaptation:} Plot each participant's task times in order for each condition.
	\item \textbf{Retention in Repeated Tasks:} For the tasks that were repeated, we will compare each participant's performance on first attempt vs second attempt.
	\item We will also analyze if certain task categories saw more improvement than others.
\end{itemize}

\subsection{Subjective Feedback Analysis}

The Likert scale responses will be summarized (e.g., average satisfaction score for AI CLI vs traditional CLI). We anticipate higher satisfaction for the AI-assisted CLI if it indeed makes tasks easier. We will use a paired test on these ratings as well.
