\section{Test Environment \& Setup}

We will use a within-subjects experimental design where each participant performs tasks under both conditions: (A) using the traditional CLI and (B) using aiman. In other words, every participant experiences both interfaces, allowing direct comparison of their performance with and without AI support. This design controls for individual differences in skill, since each person serves as their own control.

To avoid order effects (learning or fatigue influencing results), the sequence of conditions will be counterbalanced:
\begin{itemize}
	\item Half of the participants will use the Traditional CLI first, then aiman.
	\item The other half will use aiman first, then the Traditional CLI.
\end{itemize}

This counterbalancing ensures that any overall improvement due to practice is distributed across both conditions, and we can more confidently attribute differences in performance to the interface rather than task familiarity.

\subsection{Participants}

Following the recruitment strategy outlined in Chapter 3, we successfully recruited 6 participants who met our target criteria. The actual participant demographics are summarized below:

\textbf{Demographics:}
\begin{itemize}
	\item \textbf{Age Distribution:} 4 participants aged 25–34, 2 participants aged 35–44
	\item \textbf{Gender:} 4 male, 2 female participants
	\item \textbf{Education:} 3 Bachelor's degrees, 2 Master's degrees, 1 High school
	\item \textbf{Professional Experience:} 5 participants with 5–10 years experience, 1 participant with 3–5 years experience
	\item \textbf{CLI Proficiency:} Self-rated on 1-5 scale: 2 participants (level 2), 3 participants (level 3), 1 participant (level 4)
	\item \textbf{Condition Order:} Counterbalanced with 3 participants starting with traditional CLI first, 3 starting with AI-assisted CLI first
\end{itemize}

All participants had daily or weekly CLI usage experience and basic terminal familiarity as required by our selection criteria (see Chapter 3, Section 3.2.2). The sample size of 6 participants proved sufficient for meaningful statistical comparisons in our within-subjects design, where each participant served as their own control.

\subsection{Experimental Setup}

The experimental apparatus and environment are described in detail in Chapter 3, Section 3.2.3. For the actual experimental sessions, each participant session followed a standardized protocol:

\textbf{Session Preparation:} Before each session, the Docker container was reset to a clean state, clearing command history and restoring the file system to its initial configuration with all necessary test files and directories.

\textbf{Environment Consistency:} All 6 participants used the identical Linux VM environment accessed via SSH, ensuring consistent testing conditions across all sessions.

\textbf{Data Collection:} The interactive testing harness automatically logged all command inputs, outputs, timing information, and AI interactions in JSON format for subsequent analysis.

\subsection{Data Collection}

The data collection methodology and evaluation metrics are detailed in Chapter 3, Section 3.3. During the experimental sessions, all metrics were automatically collected through the testing harness, including:

\begin{itemize}
	\item Task completion times for each command scenario
	\item Success rates and error counts per participant and condition
	\item Command attempt logs with timestamps and error classifications
	\item AI interaction data (suggestions provided, accepted, or ignored)
	\item Post-session satisfaction questionnaire responses
\end{itemize}

This comprehensive data collection enabled detailed analysis of performance differences between traditional CLI and AI-assisted CLI conditions.

\section{User Testing Scenarios}

This section presents the task-based experimental framework developed to evaluate the impact of aiman interactions on user performance during command repair. The task set was constructed to reflect common command-line operations—such as file navigation, text processing, and system inspection—based on empirical usage patterns identified in large-scale studies of Unix shell behavior and established pedagogical frameworks.

Each task includes a broken command intentionally designed to elicit a syntax or semantic error. These errors were informed by prior human-computer interaction research that identified common CLI pitfalls for novice users, including syntactic memorization burdens, abstract command mappings, and high failure rates in manual correction.

The actual study used 13 carefully selected scenarios that represent common CLI operations and error patterns. Each scenario includes:
\begin{itemize}
	\item A brief description of the user's intent
	\item The incorrect (broken) command
	\item One or more valid corrected commands
	\item The skill or concept being tested
\end{itemize}

\subsection{File Navigation Scenarios}

\textbf{Scenario 1: Change to directory with spaces}
\begin{itemize}
	\item \textit{Intent:} Navigate to a directory named 'Project Files' which contains spaces
	\item \textit{Broken Command:} \texttt{cd Project\textbackslash Files}
	\item \textit{Correct Commands:} \texttt{cd "Project Files"}, \texttt{cd 'Project Files'}, \texttt{cd Project\textbackslash\textbackslash Files}
	\item \textit{Tests:} Proper handling of spaces in directory names
\end{itemize}

\textbf{Scenario 2: List files by size}
\begin{itemize}
	\item \textit{Intent:} List all files sorted by size (largest first) with human-readable sizes
	\item \textit{Broken Command:} \texttt{ls -lS --size-human}
	\item \textit{Correct Commands:} \texttt{ls -lhS}, \texttt{ls -lah --sort=size}, \texttt{ls -la --sort=size -h}
	\item \textit{Tests:} Combining flags for sorting and human-readable output
\end{itemize}

\subsection{File Management Scenarios}

\textbf{Scenario 3: Find and remove empty directories}
\begin{itemize}
	\item \textit{Intent:} Find and remove all empty directories under the current directory
	\item \textit{Broken Command:} \texttt{find . -type d -empty --delete}
	\item \textit{Correct Commands:} \texttt{find . -type d -empty -delete}, \texttt{find . -depth -type d -empty -delete}
	\item \textit{Tests:} Correct flag syntax for find command operations
\end{itemize}

\textbf{Scenario 4: Create nested directories}
\begin{itemize}
	\item \textit{Intent:} Create a directory structure for a new project, but only create directories that don't already exist
	\item \textit{Broken Command:} \texttt{mkdir src/components/buttons src/components/forms src/utils}
	\item \textit{Correct Commands:} \texttt{mkdir -p src/components/buttons src/components/forms src/utils}, \texttt{mkdir -p src/components/\{buttons,forms\} src/utils}
	\item \textit{Tests:} Using -p flag for parent directory creation
\end{itemize}

\textbf{Scenario 5: Safe file copying}
\begin{itemize}
	\item \textit{Intent:} Copy file.txt to backup.txt interactively
	\item \textit{Broken Command:} \texttt{cp -i backup.txt file.txt}
	\item \textit{Correct Commands:} \texttt{cp -i file.txt backup.txt}, \texttt{cp --interactive file.txt backup.txt}
	\item \textit{Tests:} Correct argument order for file operations
\end{itemize}

\textbf{Scenario 6: Copy unique lines}
\begin{itemize}
	\item \textit{Intent:} Copy only unique lines from file1.txt to file2.txt (removing duplicates)
	\item \textit{Broken Command:} \texttt{sort file1.txt | unique > file2.txt}
	\item \textit{Correct Commands:} \texttt{sort file1.txt | uniq > file2.txt}, \texttt{sort -u file1.txt > file2.txt}
	\item \textit{Tests:} Correct command name and usage for duplicate removal
\end{itemize}

\subsection{File Search and Text Search Scenarios}

\textbf{Scenario 7: Find recent text files}
\begin{itemize}
	\item \textit{Intent:} Find all .txt files modified in the last 7 days
	\item \textit{Broken Command:} \texttt{find . -name "*.txt" -mtime < 7}
	\item \textit{Correct Commands:} \texttt{find . -name "*.txt" -mtime -7}, \texttt{find . -type f -name "*.txt" -mtime -7}
	\item \textit{Tests:} Correct syntax for time-based file searches
\end{itemize}

\textbf{Scenario 8: Search with context}
\begin{itemize}
	\item \textit{Intent:} Search for the word "error" in log files, showing 2 lines of context before and after each match
	\item \textit{Broken Command:} \texttt{grep -c 2 "error" *.log}
	\item \textit{Correct Commands:} \texttt{grep -B 2 -A 2 "error" *.log}, \texttt{grep -C 2 "error" *.log}
	\item \textit{Tests:} Context flags versus count flags in grep
\end{itemize}

\subsection{File Viewing Scenarios}

\textbf{Scenario 9: View specific line range}
\begin{itemize}
	\item \textit{Intent:} Extract lines 7-12 from a log file
	\item \textit{Broken Command:} \texttt{head -n 12 log.txt | tail --n 6}
	\item \textit{Correct Commands:} \texttt{head -n 12 log.txt | tail -n 6}, \texttt{sed -n "7,12p" log.txt}
	\item \textit{Tests:} Correct flag syntax and command chaining
\end{itemize}

\subsection{Text Processing Scenarios}

\textbf{Scenario 10: Count non-empty lines}
\begin{itemize}
	\item \textit{Intent:} Count the number of non-empty lines in the log file
	\item \textit{Broken Command:} \texttt{grep -v \^{}\$ logfle.txt | wc -l}
	\item \textit{Correct Commands:} \texttt{grep -v "\^{}\$" logfile.txt | wc -l}, \texttt{awk "NF" logfile.txt | wc -l}
	\item \textit{Tests:} Filename typos and regex quoting
\end{itemize}

\textbf{Scenario 11: Sort CSV by numeric column}
\begin{itemize}
	\item \textit{Intent:} Sort this CSV file by the numeric values in the second column (descending order)
	\item \textit{Broken Command:} \texttt{sort -c2,2nr data.csv}
	\item \textit{Correct Commands:} \texttt{sort -t, -k2,2nr data.csv}, \texttt{sort -t, -k2nr data.csv}
	\item \textit{Tests:} Field separator specification and key selection
\end{itemize}

\subsection{System Information Scenarios}

\textbf{Scenario 12: Find largest subdirectories}
\begin{itemize}
	\item \textit{Intent:} Find the top 3 largest subdirectories and show their sizes in human-readable format
	\item \textit{Broken Command:} \texttt{du -h --depth=1 | sort -hr | head --3}
	\item \textit{Correct Commands:} \texttt{du -h --max-depth=1 | sort -hr | head -3}, \texttt{du -sh */ | sort -hr | head -3}
	\item \textit{Tests:} Correct flag names and argument syntax
\end{itemize}

\textbf{Scenario 13: Find low disk space}
\begin{itemize}
	\item \textit{Intent:} Identify filesystems with less than 20\% free space remaining (more than 80\% used)
	\item \textit{Broken Command:} \texttt{df -h | awk '\$5 > 80\%'}
	\item \textit{Correct Commands:} \texttt{df -h | awk '\$5 > "80\%"'}, \texttt{df -h | awk 'NR>1 \&\& \$5+0 > 80'}
	\item \textit{Tests:} String comparison and numeric extraction in awk
\end{itemize}

\subsection{Process Management Scenarios}

\textbf{Scenario 14: Find memory-intensive processes}
\begin{itemize}
	\item \textit{Intent:} Find the top 5 processes consuming the most memory
	\item \textit{Broken Command:} \texttt{ps aux | sorter --k4 -r | head -5}
	\item \textit{Correct Commands:} \texttt{ps aux --sort=-\%mem | head -5}, \texttt{ps aux | sort -k4nr | head -5}
	\item \textit{Tests:} Command name correction and sorting by memory usage
\end{itemize}

Each scenario was designed to test specific CLI competencies while representing realistic tasks that software engineers encounter in their daily work. The broken commands contain common error patterns identified in CLI usage studies, including flag syntax errors, argument order mistakes, and command name confusion.

\section{Procedure Flow}

Each participant will go through the following phases:

\textbf{Introduction \& Consent:} The researcher welcomes the participant, explains the study procedure, and obtains informed consent. Basic demographic and experience information is collected (if not already via a pre-survey).

\textbf{Training/Tutorial:} Before each interface, participants get a brief tutorial:
\begin{itemize}
	\item For aiman, they will be shown how the tool works: for example, how it provides suggestions after an error, how to accept a suggestion, and any special commands to invoke help. They may practice with one simple example task to familiarize themselves with the AI features.
	\item The traditional CLI likely needs less introduction, but to ensure fairness, participants can be reminded of available manual help resources (e.g. man pages or --help flags) if they get stuck.
\end{itemize}

\textbf{Baseline Task Set (Traditional CLI):} In one of the two sessions (depending on counterbalancing), the participant uses the normal terminal to attempt a set of tasks (e.g. Tasks 1–11). They will:
\begin{itemize}
	\item Read the task description provided by the experimenter or shown on-screen
	\item Attempt to execute the task in the terminal. They may use any knowledge they have, including --help or manual pages, but no AI assistance is available
	\item The system will log the time taken and whether the outcome is correct
	\item If they are truly stuck, they are allowed to ask for a hint or give up, but this will be recorded (with a reasonable maximum time per task, such as 5 minutes, to avoid endless frustration)
\end{itemize}

\textbf{Experimental Task Set (aiman):} In the other session, the participant uses the AI-enabled terminal to perform the same set of tasks using the aiman testing harness. The key difference is that AI assistance is provided reactively when command errors occur:

\begin{itemize}
	\item Participants receive the same task descriptions and attempt to execute commands normally
	\item When a command fails (returns non-zero exit code), the aiman system automatically detects the failure and provides AI-generated suggestions for correction
	\item The AI assistance includes:
	\begin{itemize}
		\item Error explanation describing why the command failed
		\item Corrected command suggestion with proper syntax
		\item Brief explanation of the correction
		\item Additional tips for similar commands
	\end{itemize}
	\item Participants can choose to:
	\begin{itemize}
		\item Utilize the suggestion as is by copying and pasting it into the terminal
		\item Modify the suggestion before executing
		\item Ignore the suggestion and try their own approach
	\end{itemize}
	\item The system logs all AI interactions including:
	\begin{itemize}
		\item When AI suggestions were triggered (command failures)
		\item What suggestions were provided
	\end{itemize}
	\item No AI assistance is provided for successful commands - the system only intervenes when errors occur
	\item The same 10-minute time limit per task applies, with the same hint/give-up options available
\end{itemize}

This reactive approach ensures that AI assistance is provided precisely when users need it most - during error states - while maintaining the natural flow of command-line interaction for successful operations.

\textbf{Post-Task Survey \& Interview:} After completing all tasks in both conditions, participants filled out a post-experiment questionnaire. The survey captured subjective feedback through the following structured questions:

\textbf{Satisfaction Questions (5-point Likert scale):}
\begin{itemize}
	\item \textit{Ease of Use:} "Which CLI was easier to use?" (1 = Traditional CLI was much easier, 5 = LLM-assisted CLI was much easier)
	\item \textit{Confidence:} "Which CLI made you feel more confident?" (1 = Much more confident with traditional CLI, 5 = Much more confident with LLM-assisted CLI)
	\item \textit{Frustration:} "Which CLI was more frustrating to use?" (1 = Traditional CLI was much more frustrating, 5 = LLM-assisted CLI was much more frustrating)
\end{itemize}

\textbf{Additional Feedback:}
\begin{itemize}
	\item \textit{Open-ended Comments:} "Please provide any additional comments or feedback about your experience with both CLIs:"
\end{itemize}

The questionnaire was administered through an interactive terminal interface, with responses automatically logged to the JSON data storage system. No separate structured interview was conducted beyond the questionnaire responses, though participants were encouraged to provide detailed feedback in the open-ended comments section.

Throughout the sessions, the experimenter will observe and take notes (without interfering). Notable observations, such as signs of frustration (sighs, repeated errors) or ease (quick task completion), will be recorded as qualitative data.

\section{Analysis of Results}

Once the data is collected, we will perform both quantitative and qualitative analysis to address the research goals.

\subsection{Efficiency Analysis}

For each task, we'll compare how long it took to complete using the traditional CLI versus aiman. Since the same people use both, we'll do paired comparisons—like a paired t-test (if the data looks normal) or the Wilcoxon signed-rank test (if not)—to see if the difference in times is significant.

We expect aiman to help more with harder tasks, especially those with tricky syntax or ones where people usually pause to think. For simple tasks (like listing files), the difference might be small.

\subsection{Effectiveness and Error Analysis}

We will examine success rates and error counts:
\begin{itemize}
	\item Success rate likely will be high for both (since users can eventually complete tasks), but the number of initial errors and retries will differ.
	\item We'll calculate the average number of errors per task in each condition.
	\item Using the logs, we might also classify error types (e.g., syntax errors, wrong command approach, etc.) to see what kinds of mistakes the AI helps with most.
\end{itemize}



\subsection{Subjective Feedback Analysis}

The Likert scale responses will be summarized (e.g., average satisfaction score for aiman vs traditional CLI). We anticipate higher satisfaction for aiman if it indeed makes tasks easier. We will use a paired test on these ratings as well.
